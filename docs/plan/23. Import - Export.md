## 23. Import / Export

> **MVP Status**: PARTIALLY IMPLEMENTED  
> Export functionality is implemented for JSON and CSV formats.  
> Import functionality is implemented with validation and preview.  
> This document serves as both specification and implementation reference.

### 23.1 Overview

The Import / Export subsystem enables users to extract AshTrail data for external analysis, backup, or migration, and to re-ingest previously exported data back into the app.

This subsystem exists to:
- Provide user-owned data portability
- Support manual backups and restores
- Enable analysis outside the app (e.g., spreadsheets, scripts)
- Allow migration between devices or accounts without relying on cloud sync

Import / Export is explicitly **offline-first** and operates on **local persisted data**.

---

### 23.2 Responsibilities

- Serialize persisted domain data into supported export formats
- Scope exports to a single logical account
- Validate imported data before mutation
- Prevent corruption of existing data
- Surface partial or failed imports clearly to the user
- Avoid implicit merging or destructive overwrites

Non-responsibilities:
- No automatic cloud uploads
- No background sync
- No cross-account merging
- No schema migration beyond basic validation

---

### 23.3 Data Structures

#### 23.3.1 Export Payload (Logical Model)

Exports are **self-describing** and include minimal metadata.

| Field            | Type        | Required | Notes |
|------------------|------------|----------|------|
| `exportVersion`  | string     | yes      | Semantic version of export schema |
| `exportedAt`     | datetime   | yes      | ISO-8601 timestamp |
| `accountId`      | string     | yes      | Logical account identifier |
| `accountName`    | string     | yes      | Human-readable account name |
| `logs`           | array      | yes      | Log records for the account |
| `settings`       | object?    | no       | Account-scoped settings (if applicable) |

Constraints:
- `logs` must be an array (may be empty)
- `accountId` must match a single account scope
- Unknown fields must be ignored on import

---

### 23.4 Export Formats

#### 23.4.1 CSV

CSV exports are intended for:
- Spreadsheet analysis
- Lightweight data sharing
- Human inspection

Characteristics:
- One row per log entry
- Flat structure (no nested objects)
- Header row included

Example columns:
- `logId`
- `timestamp`
- `value`
- `notes`
- `createdAt`
- `updatedAt`

Constraints:
- Timestamps serialized as ISO-8601 strings
- Null values serialized as empty fields
- Ordering is chronological (ascending by timestamp)

Limitations:
- Lossy compared to JSON
- Cannot represent nested or future fields cleanly

---

#### 23.4.2 JSON

JSON exports are intended for:
- Full-fidelity backups
- Round-trip import
- Programmatic processing

Characteristics:
- Preserves full object structure
- Includes metadata envelope
- Forward-compatible via ignored unknown fields

Constraints:
- UTF-8 encoded
- Pretty-printing optional (implementation detail)
- Must include `exportVersion`

---

### 23.5 Export Scope

#### 23.5.1 Per Account

Exports are scoped to **exactly one account**.

Rules:
- Only data belonging to the selected account is exported
- No cross-account references are included
- Global app state is excluded

Rationale:
- Prevents accidental data leakage
- Simplifies import semantics
- Aligns with account switching model

---

### 23.6 Import Rules

#### 23.6.1 Validation

All imports must pass validation **before** any data is written.

Validation steps:
1. File format validation (CSV vs JSON)
2. Schema validation
3. Required field presence
4. Type validation
5. Account scope validation

Invalid imports:
- Abort before mutation
- Surface explicit error to user
- No partial writes

---

#### 23.6.2 Conflict Handling

Conflicts occur when imported data collides with existing local data.

Rules:
- Imports never overwrite existing records silently
- Conflicts are detected by stable identifiers (e.g., `logId`)

Default behavior:
- Skip conflicting records
- Import only non-conflicting records

Open question:
- Should the user be prompted to choose between:
  - Skip
  - Replace
  - Duplicate with new ID

(Currently unspecified)

---

### 23.7 State & Flow

#### 23.7.1 Export Flow

1. User selects account
2. User selects export format
3. Data is read from local persistence
4. Data is serialized
5. File is written via OS share/save mechanism

No app state mutation occurs during export.

---

#### 23.7.2 Import Flow

1. User selects file
2. File is parsed
3. Validation runs
4. Conflicts are detected
5. Import plan is constructed
6. Data is written transactionally
7. State is refreshed

Import must be **atomic per record**, not per file.

---

### 23.8 Edge Cases & Failure Modes

#### 23.8.1 Partial Imports

Partial imports may occur when:
- Some records fail validation
- Conflicts are encountered
- User cancels mid-import

Rules:
- Successfully imported records remain
- Failed records are skipped
- User receives a summary:
  - Imported count
  - Skipped count
  - Failure reasons

Partial import must never corrupt:
- Existing records
- Account metadata
- Indexes or aggregates

---

#### 23.8.2 Other Failure Modes

- Unsupported file format
- Corrupt file contents
- Version mismatch
- Out-of-memory on large imports
- Permission denied during file access

All failures must:
- Be surfaced to the user
- Leave local state unchanged (except successful partial imports)

---

### 23.9 Assumptions & Open Questions

Assumptions:
- Import/export is user-initiated only
- Files are provided via OS picker
- Export schema versioning exists

Open Questions:
- Is schema migration supported for older export versions?
- Are account settings included in JSON exports?
- Should CSV imports be supported or export-only?
- Should imports allow creating a new account automatically?

---

### 23.10 Future Extensions

- Encrypted exports
- Compressed export bundles
- Scheduled automatic backups
- Cloud storage integration
- Full account cloning via import
- Diff-based import previews

---
